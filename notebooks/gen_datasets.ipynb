{
 "cells": [
  {
   "metadata": {
    "scrolled": true,
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "# !pip install numpy requests nlpaug"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "# Install libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.word as nlpaw\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm as tqdm\n",
    "\n",
    "\n",
    "# Load the data\n",
    "train_valid = pd.read_csv('data_input/train.csv')\n",
    "test = pd.read_csv('data_input/test.csv')\n",
    "test_labels = pd.read_csv('data_input/test_labels.csv')\n",
    "\n",
    "# Check data\n",
    "print('Our (training + valid) data has ', train_valid.shape[0], ' rows.')\n",
    "print('Our test data has ', test.shape[0], ' rows.')\n",
    "print('Our test label data has ', test_labels.shape[0], ' rows.')\n",
    "\n",
    "\n",
    "\n",
    "# Allow us to see full text (not truncated)\n",
    "pd.set_option('display.max_colwidth', None)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Functions"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "def analyze_dist(df):\n    print('Total rows:           ', df.shape[0])\n    print('Clean texts:          ', df.shape[0] - df['isToxic'].sum())\n    print('Toxic texts:          ', df['isToxic'].sum())\n    print('Toxic texts make up   ', ((df['isToxic'].sum() / df.shape[0])*100).round(2), 'percent of our total data')\n    return",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "def combine_toxic_classes(df):\n    \"\"\"\"\"\"\"\"\"\n    Reconfigures the Jigsaw Toxic Comment dataset from a \n    multi-label classification problem to a\n    binary classification problem predicting if a text is\n    toxic (class=1) or non-toxic (class=0).\n    \n    Input:\n        - df:  A pandas DataFrame with columns:\n               - 'id'\n               - 'comment_text'\n               - 'toxic'\n               - 'severe_toxic'\n               - 'obscene'\n               - 'threat'\n               - 'insult'\n               - 'identity_hate'\n    Output:\n        - df:  A modified pandas DataFrame with columns:\n               - 'comment_text' containing strings of text.\n               - 'isToxic' binary target variable containing 0's and 1's.\n    \"\"\"\"\"\"\"\"\"\n    # Create a binary classification label for 'isToxic'\n    # and drop miscellaneous labels.\n    df['isToxic'] = (df['toxic']==1)\n    drop_cols = ['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n    df.drop(columns=drop_cols, inplace=True)\n    df.replace(to_replace={'isToxic': {True:1, False:0}}, inplace=True)\n    \n    # Cast column values to save memory\n    df['isToxic'] = df['isToxic'].astype('int8')\n    \n    return df",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "def undersample_majority(df, percent_conserve):\n    \"\"\"\"\"\"\"\"\"\n    Undersamples the majority class ('isToxic'==0) by conserving\n    a given percent as specified by the user.\n    \n    Inputs:\n        - df:  A pandas DataFrame with columns:\n               - 'comment_text' containing strings of text.\n               - 'isToxic' binary target variable containing 0's and 1's. \n        - percent_conserve:  Float representing fraction of \n                             majority class (clean_texts) to conserve\n    Outputs:\n        - downsampled_df:    A new pandas DataFrame that has been shuffled\n                             and has had its majority class downsampled.\n    \"\"\"\"\"\"\"\"\"\n    # Get rows of clean and toxic texts\n    clean_texts = df[df['isToxic']==0]\n    toxic_texts = df[df['isToxic']==1]\n    \n    # Randomly sample from the majority class and construct a new DataFrame\n    # consisting of the majority class (clean_texts) + the minority classes (toxic_texts)\n    to_conserve = clean_texts.sample(frac=percent_conserve, random_state=42)\n    downsampled_df = to_conserve.append(toxic_texts, ignore_index=True)\n    \n    return downsampled_df.sample(frac=1, random_state=42).reset_index(drop=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "def augment_sentence(sentence, aug, num_threads):\n    \"\"\"\"\"\"\"\"\"\n    Constructs a new sentence via text augmentation.\n    \n    Input:\n        - sentence:     A string of text\n        - aug:          An augmentation object defined by the nlpaug library\n        - num_threads:  Integer controlling the number of threads to use if\n                        augmenting text via CPU\n    Output:\n        - A string of text that been augmented\n    \"\"\"\"\"\"\"\"\"\n    return aug.augment(sentence, num_thread=num_threads)\n    \n\n\ndef augment_text(df, aug, num_threads, num_times):\n    \"\"\"\"\"\"\"\"\"\n    Takes a pandas DataFrame and augments its text data.\n    \n    Input:\n        - df:            A pandas DataFrame containing the columns:\n                                - 'comment_text' containing strings of text to augment.\n                                - 'isToxic' binary target variable containing 0's and 1's.\n        - aug:           Augmentation object defined by the nlpaug library.\n        - num_threads:   Integer controlling number of threads to use if augmenting\n                         text via CPU\n        - num_times:     Integer representing the number of times to augment text.\n    Output:\n        - df:            The same pandas DataFrame with augmented data appended to it\n                         and with rows randomly shuffled.\n    \"\"\"\"\"\"\"\"\"\n    # Get rows of data to augment\n    to_augment = df[df['isToxic']==1]\n    to_augmentX = to_augment['comment_text']\n    to_augmentY = np.ones(len(to_augmentX.index) * num_times, dtype=np.int8)\n    \n    # Build up dictionary containing augmented data\n    aug_dict = {'comment_text':[], 'isToxic':to_augmentY}\n    for i in tqdm.tqdm(range(num_times)):\n        augX = [augment_sentence(x, aug, num_threads) for x in to_augmentX]\n        aug_dict['comment_text'].extend(augX)\n    \n    # Build DataFrame containing augmented data\n    aug_df = pd.DataFrame.from_dict(aug_dict)\n    \n    return df.append(aug_df, ignore_index=True).sample(frac=1, random_state=42)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate Training and Validation Datasets"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Convert from multi-label --> binary classification\ntrain_valid = combine_toxic_classes(train_valid)\n\n# Undersample majority class (class=0)\nunbalanced_df = undersample_majority(train_valid, .42)\n\n# Upsample minority class (class=1) to create a roughly 50-50 class distribution\n# aug5p = nlpaw.ContextualWordEmbsAug(model_path='bert-base-uncased', aug_min=1, aug_p=0.05, action=\"substitute\")\nbalanced_df = pd.read_csv('data_input/downsample_aug.csv', index_col=0)\n# balanced_df = augment_text(downsampled_df, aug5p, 8, 3)\n\n# Generate 80-20 train-validation splits\nX_train, X_valid, y_train, y_valid = train_test_split(unbalanced_df['comment_text'],\n                                                    unbalanced_df['isToxic'],\n                                                    train_size=0.8,\n                                                    stratify=unbalanced_df['isToxic'],\n                                                    shuffle=True,\n                                                    random_state=42)\n\nX_train_aug, X_valid_aug, y_train_aug, y_valid_aug = train_test_split(balanced_df['comment_text'],\n                                                                    balanced_df['isToxic'],\n                                                                    train_size=0.8,\n                                                                    stratify=balanced_df['isToxic'],\n                                                                    shuffle=True,\n                                                                    random_state=42)\n\n# Output unbalanced data\nX_train.to_csv('data_output/unbalanced_splits/X_train.csv', index=False)\nX_valid.to_csv('data_output/unbalanced_splits/X_valid.csv', index=False)\ny_train.to_csv('data_output/unbalanced_splits/y_train.csv', index=False)\ny_valid.to_csv('data_output/unbalanced_splits/y_valid.csv', index=False)\n\n# Output balanced data\nX_train_aug.to_csv('data_output/balanced_splits/X_train_aug.csv', index=False)\nX_valid_aug.to_csv('data_output/balanced_splits/X_valid_aug.csv', index=False)\ny_train_aug.to_csv('data_output/balanced_splits/y_train_aug.csv', index=False)\ny_valid_aug.to_csv('data_output/balanced_splits/y_valid_aug.csv', index=False)\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate Test Dataset"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Read in the data\ntest_labels = pd.read_csv('data_input/test_labels.csv')\ntest = pd.read_csv('data_input/test.csv')\n\n# Merge test text data with its labels\ntest_merged = test.merge(test_labels, on='id', how='left')\n\n# Remove masked rows that will not be tested\ntest_merged = test_merged[test_merged['toxic'] != -1]\n\n# Convert from multi-label --> binary classification \ntest_merged = combine_toxic_classes(test_merged)\n\n# Output data\ntest_merged.to_csv('data_output/test_merged.csv', index=False)",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}