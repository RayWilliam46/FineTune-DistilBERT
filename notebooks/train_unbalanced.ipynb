{"cells":[{"metadata":{"title":"In [1]:","trusted":false},"cell_type":"code","source":"# !pip install --upgrade pip\n# !pip install comet_ml\n# !pip install -q pyyaml h5py\n# !pip install scikit-plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial Setup"},{"metadata":{"title":"In [2]:","trusted":false},"cell_type":"code","source":"# Import libraries\nfrom comet_ml import Experiment\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport scikitplot as skplt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import initializers\nfrom transformers import DistilBertTokenizerFast\nfrom transformers import TFDistilBertModel, DistilBertConfig\n\n# Import utility functions\nfrom src.utils.train_utils import batch_encode\nfrom src.utils.train_utils import focal_loss\n\n# Import matplotlib\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n\n# Load training data\nX_train = pd.read_csv('data/processed/unbalanced_dataset/X_train.csv')['comment_text']\nX_valid = pd.read_csv('data/processed/unbalanced_dataset/X_valid.csv')['comment_text']\ny_train = pd.read_csv('data/processed/unbalanced_dataset/y_train.csv')['isToxic']\ny_valid = pd.read_csv('data/processed/unbalanced_dataset/y_valid.csv')['isToxic']\n\n# Load test data\ntest = pd.read_csv('data/processed/test_merged.csv')\nX_test = test['comment_text']\ny_test = test['isToxic']\n\n# Check data\nprint('Our training data has   ', len(X_train.index), ' rows.')\nprint('Our validation data has ', len(X_valid.index), ' rows.')\nprint('Our test data has       ', len(X_test.index), ' rows.')\n\n\n\n# Allow us to see full text (not truncated)\npd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set Up Comet.ml Experiment"},{"metadata":{"title":"In [3]:","trusted":false},"cell_type":"code","source":"# Create an experiment with your api key:\nexperiment = Experiment(\n    api_key=\"YOUR_API_KEY\",\n    project_name=\"YOUR_PROJECT_NAME\",\n    workspace=\"YOUR_WORKSPACE\",\n    auto_histogram_weight_logging=True,\n    auto_histogram_gradient_logging=True,\n    auto_histogram_activation_logging=True,\n    auto_log_co2=True,\n    log_env_details=True,\n    log_env_gpu=True,\n    log_env_cpu=True\n)\n\n# Set parameters:\nparams = {'MAX_LENGTH': 128,\n          'EPOCHS': 6,\n          'LEARNING_RATE': 5e-5,\n          'FT_EPOCHS': 2,\n          'OPTIMIZER': 'adam',\n          'FL_GAMMA': 2.0,\n          'FL_ALPHA': 0.2,\n          'BATCH_SIZE': 64,\n          'NUM_STEPS': len(X_train.index) // 64,\n          'DISTILBERT_DROPOUT': 0.2,\n          'DISTILBERT_ATT_DROPOUT': 0.2,\n          'LAYER_DROPOUT': 0.2,\n          'KERNEL_INITIALIZER': 'GlorotNormal',\n          'BIAS_INITIALIZER': 'zeros',\n          'POS_PROBA_THRESHOLD': 0.5,          \n          'ADDED_LAYERS': 'Dense 256, Dense 32, Dropout 0.2',\n          'LR_SCHEDULE': '5e-5 for 6 epochs, Fine-tune w/ adam for 2 epochs @2e-5',\n          'FREEZING': 'All DistilBERT layers frozen for 6 epochs, then unfrozen for 2',\n          'CALLBACKS': '[early_stopping w/ patience=0]',\n          'RANDOM_STATE':42\n          }\n\n\n# Log parameters\nexperiment.log_parameters(params)\n\n# Log data assets\nexperiment.log_asset('data/processed/test_merged.csv')\nexperiment.log_asset_folder('data/processed/unbalanced_dataset')\nexperiment.log_dataset_info(name='Toxic Comment (Unbalanced)')","execution_count":null,"outputs":[]},{"metadata":{"title":"In [5]:","trusted":false},"cell_type":"code","source":"########## Ensure reproducibility ##########\n\n\n# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\nos.environ['PYTHONHASHSEED']=str(params['RANDOM_STATE'])\n\n# 2. Set `python` built-in pseudo-random generator at a fixed value\nrandom.seed(params['RANDOM_STATE'])\n\n# 3. Set `numpy` pseudo-random generator at a fixed value\nnp.random.seed(params['RANDOM_STATE'])\n\n# 4. Set `tensorflow` pseudo-random generator at a fixed value\ntf.random.set_seed(seed=params['RANDOM_STATE'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize text"},{"metadata":{"title":"In [6]:","trusted":false},"cell_type":"code","source":"# Instantiate DistilBERT tokenizer...we use the Fast version to optimize runtime\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# Encode X_train\nX_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n\n# Encode X_valid\nX_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())\n\n# Encode X_test\nX_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## Build Model"},{"metadata":{"title":"In [9]:","trusted":false},"cell_type":"code","source":"def build_model(transformer, max_length=params['MAX_LENGTH']):\n    \"\"\"\"\"\"\"\"\"\n    Template for building a model off of the BERT or DistilBERT architecture\n    for a binary classification task.\n    \n    Input:\n      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n                      with no added classification head attached.\n      - max_length:   integer controlling the maximum number of encoded tokens \n                      in a given sequence.\n    \n    Output:\n      - model:        a compiled tf.keras.Model with added classification layers \n                      on top of the base pre-trained model architecture.\n    \"\"\"\"\"\"\"\"\"\"\n    \n    # Define weight initializer with a random seed to ensure reproducibility\n    weight_initializer = tf.keras.initializers.GlorotNormal(seed=params['RANDOM_STATE']) \n    \n    # Define input layers\n    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n                                            name='input_ids', \n                                            dtype='int32')\n    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n                                                  name='input_attention', \n                                                  dtype='int32')\n    \n    # DistilBERT outputs a tuple where the first element at index 0\n    # represents the hidden-state at the output of the model's last layer.\n    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n    \n    # We only care about DistilBERT's output for the [CLS] token, which is located\n    # at index 0.  Splicing out the [CLS] tokens gives us 2D data.\n    cls_token = last_hidden_state[:, 0, :]\n    \n    D1 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n                                 seed=params['RANDOM_STATE']\n                                )(cls_token)\n    \n    X = tf.keras.layers.Dense(256,\n                              activation='relu',\n                              kernel_initializer=weight_initializer,\n                              bias_initializer='zeros'\n                              )(D1)\n    \n    D2 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n                                 seed=params['RANDOM_STATE']\n                                )(X)\n    \n    X = tf.keras.layers.Dense(32,\n                              activation='relu',\n                              kernel_initializer=weight_initializer,\n                              bias_initializer='zeros'\n                              )(D2)\n    \n    D3 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n                                 seed=params['RANDOM_STATE']\n                                )(X)\n    \n    # Define a single node that makes up the output layer (for binary classification)\n    output = tf.keras.layers.Dense(1, \n                                   activation='sigmoid',\n                                   kernel_initializer=weight_initializer,  # CONSIDER USING CONSTRAINT\n                                   bias_initializer='zeros'\n                                   )(D3)\n    \n    # Define the model\n    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n    \n    # Compile the model\n    model.compile(tf.keras.optimizers.Adam(lr=params['LEARNING_RATE']), \n                  loss=focal_loss(),\n                  metrics=['accuracy'])\n    \n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"# The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n# and without any specific head on top.\nconfig = DistilBertConfig(dropout=params['DISTILBERT_DROPOUT'], \n                          attention_dropout=params['DISTILBERT_ATT_DROPOUT'], \n                          output_hidden_states=True)\ndistilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n\n# Freeze DistilBERT layers to preserve pre-trained weights \nfor layer in distilBERT.layers:\n    layer.trainable = False\n\n# Build model\nmodel = build_model(distilBERT)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Weights of Added Layers and Classification Head"},{"metadata":{"title":"In [10]:","trusted":false},"cell_type":"code","source":"# Define callbacks\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                  mode='min',\n                                                  min_delta=0,\n                                                  patience=0,\n                                                  restore_best_weights=True)\n\n# Train the model\ntrain_history1 = model.fit(\n    x = [X_train_ids, X_train_attention],\n    y = y_train.to_numpy(),\n    epochs = params['EPOCHS'],\n    batch_size = params['BATCH_SIZE'],\n    steps_per_epoch = params['NUM_STEPS'],\n    validation_data = ([X_valid_ids, X_valid_attention], y_valid.to_numpy()),\n    callbacks=[early_stopping],\n    verbose=2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unfreeze DistilBERT and Fine-tune All Weights"},{"metadata":{"title":"In [12]:","trusted":false},"cell_type":"code","source":"# Unfreeze DistilBERT weights to enable fine-tuning\nfor layer in distilBERT.layers:\n    layer.trainable = True\n\n# Lower the learning rate to prevent destruction of pre-trained weights\noptimizer = tf.keras.optimizers.Adam(lr=2e-5)\n\n# Recompile model after unfreezing\nmodel.compile(optimizer=optimizer, \n              loss=focal_loss(),\n              metrics=['accuracy'])\n\n# Train the model\ntrain_history2 = model.fit(\n    x = [X_train_ids, X_train_attention],\n    y = y_train.to_numpy(),\n    epochs = params['FT_EPOCHS'],\n    batch_size = params['BATCH_SIZE'],\n    steps_per_epoch = params['NUM_STEPS'],\n    validation_data = ([X_valid_ids, X_valid_attention], y_valid.to_numpy()),\n    callbacks=[early_stopping],\n    verbose=2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate Model Predictions"},{"metadata":{"title":"In [17]:","trusted":false},"cell_type":"code","source":"with experiment.test():\n    # Generate predictions\n    y_pred = model.predict([X_test_ids, X_test_attention])\n    y_pred_thresh = np.where(y_pred >= params['POS_PROBA_THRESHOLD'], 1, 0)\n    \n    # Get evaluation results\n    accuracy = accuracy_score(y_test, y_pred_thresh)\n    auc_roc = roc_auc_score(y_test, y_pred)\n    \n    # Log evaluation metrics\n    experiment.log_metrics({'Accuracy':accuracy, 'AUC-ROC':auc_roc})\n    \n    # Log the ROC curve\n    fpr, tpr, thresholds = roc_curve(y_test.to_numpy(), y_pred)\n    experiment.log_curve('ROC cuve', fpr, tpr)\n\n\nprint('Accuracy:  ', accuracy)   # 0.9218\nprint('ROC-AUC:   ', auc_roc)    # 0.9691","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot Training and Validation Loss"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"# Build train_history\nhistory_df1 = pd.DataFrame(train_history1.history)\nhistory_df2 = pd.DataFrame(train_history2.history)\nhistory_df = history_df1.append(history_df2, ignore_index=True)\n\n# Plot training and validation loss over each epoch\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nplt.title(label='Training + Validation Loss Over Time', fontsize=17, pad=19)\nplt.xlabel('Epoch', labelpad=14, fontsize=14)\nplt.ylabel('Focal Loss', labelpad=16, fontsize=14)\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\n# Save figure\nplt.savefig('figures/unbalanced_trainvalloss.png', dpi=300.0, transparent=True)\n\n# Log the figure\nexperiment.log_image('figures/unbalanced_trainvalloss.png', name='Train Validation Loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot the Confusion Matrix"},{"metadata":{"title":"In [18]:","trusted":false},"cell_type":"code","source":"# Plot confusion matrix\nskplt.metrics.plot_confusion_matrix(y_test.to_list(),\n                                    y_pred_thresh.tolist(),\n                                    figsize=(6,6),\n                                    text_fontsize=14)\nplt.title(label='Test Confusion Matrix', fontsize=20, pad=17)\nplt.xlabel('Predicted Label', labelpad=14)\nplt.ylabel('True Label', labelpad=14)\n\n# Save the figure\nplt.savefig('figures/unbalanced_confusionmatrix.png', dpi=300.0, transparent=True)\n\n# Log the confusion matrix\nexperiment.log_image('figures/unbalanced_confusionmatrix.png', name='Test Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## End the Experiment"},{"metadata":{"title":"In [19]:","pycharm":{"name":"#%%\n"},"trusted":false},"cell_type":"code","source":"# Save model\ntf.saved_model.save(model, 'models/unbalanced_model')\n\n# Log final model\nexperiment.log_model(name='Final Model (Unbalanced)', \n                     file_or_folder='models/unbalanced_model')\n\n# End Comet.ml experiment\nexperiment.end()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}