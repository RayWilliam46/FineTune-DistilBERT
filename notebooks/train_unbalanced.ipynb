{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"title":"In [1]:"},"outputs":[],"source":["get_ipython().system('pip install --upgrade pip')\n","get_ipython().system('pip install comet_ml')\n","get_ipython().system('pip install -q pyyaml h5py')\n","get_ipython().system('pip install scikit-plot')\n","get_ipython().system('cp -r ../input/keras-one-cycle/keras-one-cycle-master/* ./')"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [2]:"},"outputs":[],"source":["# Import libraries\n","from comet_ml import Experiment\n","from clr import OneCycleLR\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","import seaborn as sns\n","import scikitplot as skplt\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import initializers\n","import tqdm\n","\n","# Import matplotlib\n","pd.plotting.register_matplotlib_converters()\n","import matplotlib.pyplot as plt\n","get_ipython().run_line_magic('matplotlib', 'inline')\n","\n","# Load training data\n","X_train = pd.read_csv('../input/toxic-comment-unbalanced/X_train.csv')['comment_text']\n","X_valid = pd.read_csv('../input/toxic-comment-unbalanced/X_valid.csv')['comment_text']\n","y_train = pd.read_csv('../input/toxic-comment-unbalanced/y_train.csv')['isToxic']\n","y_valid = pd.read_csv('../input/toxic-comment-unbalanced/y_valid.csv')['isToxic']\n","\n","# Load test data\n","test = pd.read_csv('../input/toxic-comment-test-merged/test_merged.csv')\n","X_test = test['comment_text']\n","y_test = test['isToxic']\n","\n","# Check data\n","print('Our training data has   ', len(X_train.index), ' rows.')\n","print('Our validation data has ', len(X_valid.index), ' rows.')\n","print('Our test data has       ', len(X_test.index), ' rows.')\n","\n","\n","# Allow us to see full text (not truncated)\n","pd.set_option('display.max_colwidth', None)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [3]:"},"outputs":[],"source":["# Create an experiment with your api key:\n","experiment = Experiment(\n","    api_key=\"TJ15ARkbpofBsCW8nh6bFXun5\",\n","    project_name=\"toxic-comment-unbalanced\",\n","    workspace=\"raywilliamcs\",\n","    auto_histogram_weight_logging=True,\n","    auto_histogram_gradient_logging=True,\n","    auto_histogram_activation_logging=True,\n","    auto_log_co2=True,\n","    log_env_details=True,\n","    log_env_gpu=True,\n","    log_env_cpu=True\n",")\n","\n","# Log data assets\n","experiment.log_asset_folder('../input/toxic-comment-test-merged')\n","experiment.log_asset_folder('../input/toxic-comment-unbalanced')\n","experiment.log_dataset_info(name='Toxic Comment (Unbalanced)')"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [4]:"},"outputs":[],"source":["# Set parameters:\n","params = {'MAX_LENGTH': 128,\n","          'EPOCHS': 6,\n","          'LEARNING_RATE': 5e-5,\n","          \n","          'FT_EPOCHS': 2,\n","#           'MAX_LR': 5e-5,\n","#           'MAX_MOMENT': 0.95,\n","#           'MIN_MOMENT':0.85,\n","          'OPTIMIZER': 'adam',\n","          'LOSS': 'Focal Loss // gamma=2, alpha=.2',\n","          'FL_GAMMA': 2.0,\n","          'FL_ALPHA': 0.2,\n","          'BATCH_SIZE': 64,\n","          'NUM_STEPS': len(X_train.index) // 64,\n","          'DISTILBERT_DROPOUT': 0.2,\n","          'DISTILBERT_ATT_DROPOUT': 0.2,\n","          'LAYER_DROPOUT': 0.2,\n","          'KERNEL_INITIALIZER': 'GlorotNormal',\n","          'BIAS_INITIALIZER': 'zeros',\n","          'POS_PROBA_THRESHOLD': 0.5,\n","          'CALLBACKS':'[early_stopping w/ patience=0]',\n","          \n","          'ADDED_LAYERS': 'Dense 256, Dense 32, Dropout 0.2',\n","          'LR_SCHEDULE': '5e-5 for 6 epochs, Fine-tune w/ adam for 4 epochs @2e-5',\n","          'FREEZING': 'All distilBERT layers frozen for 6 epochs, then unfrozen for 4',\n","          'OTHER': 'FT_EPOCHS 4-->2, early_stopping w/ patience=0',\n","          'RANDOM_STATE':42\n","          }\n","\n","\n","# Log parameters:\n","experiment.log_parameters(params)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [5]:"},"outputs":[],"source":["########## Ensure reproducibility ##########\n","\n","\n","# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n","os.environ['PYTHONHASHSEED']=str(params['RANDOM_STATE'])\n","\n","# 2. Set `python` built-in pseudo-random generator at a fixed value\n","random.seed(params['RANDOM_STATE'])\n","\n","# 3. Set `numpy` pseudo-random generator at a fixed value\n","np.random.seed(params['RANDOM_STATE'])\n","\n","# 4. Set `tensorflow` pseudo-random generator at a fixed value\n","tf.random.set_seed(seed=params['RANDOM_STATE'])"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [6]:"},"outputs":[],"source":["# DistilBERT and BERT use the same tokenizer...we use the Fast version to optimize runtime\n","from transformers import DistilBertTokenizerFast\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","def batch_encode(tokenizer, sentences):\n","    \"\"\"\"\"\"\"\"\"\n","    A function that encodes a batch of sentences and returns the sentences'\n","    corresponding encodings and attention masks that are ready to be fed \n","    into a pre-trained transformer model.\n","    \n","    \n","    Input:\n","        - tokenizer:  tokenizer object from the PreTrainedTokenizer Class\n","        - sentences:  a list of strings where each string represents a sentence\n","    Output:\n","        - input_ids:       a sentence encoded as a tf.Tensor object\n","        - attention_mask:  the sentence's attention mask encoded as a tf.Tensor object\n","    \"\"\"\"\"\"\"\"\"\n","    inputs = tokenizer.batch_encode_plus(sentences,\n","                                         max_length=params['MAX_LENGTH'],\n","                                         padding='longest', # implements dynamic padding\n","                                         truncation=True,\n","                                         return_tensors='tf',\n","                                         return_attention_mask=True, \n","                                         return_token_type_ids=False)\n","    \n","    input_ids = inputs['input_ids']\n","    attention_mask = inputs['attention_mask']\n","    \n","    return input_ids, attention_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [7]:"},"outputs":[],"source":["# Encode X_train\n","X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n","\n","# Encode X_valid\n","X_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())\n","\n","# Encode X_test\n","X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [8]:"},"outputs":[],"source":["from transformers import TFDistilBertModel, DistilBertConfig\n","\n","# The bare DistilBERT encoder/transformer outputting raw hidden-states \n","# without any specific head on top.  Enumerates default settings. \n","config = DistilBertConfig(dropout=params['DISTILBERT_DROPOUT'], \n","                          attention_dropout=params['DISTILBERT_ATT_DROPOUT'], \n","                          output_hidden_states=True)\n","distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n","\n","# Make DistilBERT layers untrainable\n","for layer in distilBERT.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [9]:"},"outputs":[],"source":["def focal_loss(gamma=params['FL_GAMMA'], alpha=params['FL_ALPHA']):\n","    def focal_loss_fixed(y_true, y_pred):\n","        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n","        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n","        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n","    return focal_loss_fixed\n","\n","def build_model(transformer, max_length=params['MAX_LENGTH']):\n","    \n","    # Define weight initializer with a random seed to ensure reproducibility\n","    weight_initializer = tf.keras.initializers.GlorotNormal(seed=params['RANDOM_STATE']) \n","    \n","    # Define input layers\n","    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n","                                            name='input_ids', \n","                                            dtype='int32')\n","    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n","                                                  name='input_attention', \n","                                                  dtype='int32')\n","    \n","    # DistilBERT outputs a tuple where the first element is tf.Tensor of shape \n","    # (batch_size, sequence_length, hidden_size=768).\n","    # The tf.Tensor represents a sequence of hidden-states at the output of the \n","    # last layer of the model.\n","    last_hidden_states = transformer([input_ids_layer, input_attention_layer])[0]\n","    \n","    # We only care about DistilBERT's output for the [CLS] token, which is located\n","    # at index 0.  Splicing out the [CLS] tokens gives us 2D data.\n","    cls_token = last_hidden_states[:, 0, :]\n","    \n","    D1 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n","                                 seed=params['RANDOM_STATE']\n","                                )(cls_token)\n","    \n","    X = tf.keras.layers.Dense(256,\n","                              activation='relu',\n","                              kernel_initializer=weight_initializer,\n","                              bias_initializer='zeros'\n","                              )(D1)\n","    \n","    D2 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n","                                 seed=params['RANDOM_STATE']\n","                                )(X)\n","    \n","    X = tf.keras.layers.Dense(32,\n","                              activation='relu',\n","                              kernel_initializer=weight_initializer,\n","                              bias_initializer='zeros'\n","                              )(D2)\n","    \n","    D3 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n","                                 seed=params['RANDOM_STATE']\n","                                )(X)\n","    \n","    # Define a single node that makes up the output layer (for binary classification)\n","    output = tf.keras.layers.Dense(1, \n","                                   activation='sigmoid',\n","                                   kernel_initializer=weight_initializer,  # CONSIDER USING CONSTRAINT\n","                                   bias_initializer='zeros'\n","                                   )(D3)\n","    \n","    # Define the model\n","    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n","    \n","    # Compile the model\n","    model.compile(tf.keras.optimizers.Adam(lr=params['LEARNING_RATE']), \n","                  loss=focal_loss(),\n","                  metrics=['accuracy'])\n","    \n","    return model\n","    \n","    \n","model = build_model(distilBERT)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [10]:"},"outputs":[],"source":["# Define callbacks\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n","                                                  mode='min',\n","                                                  min_delta=0,\n","                                                  patience=0,\n","                                                  restore_best_weights=True)\n","\n","# one_cycle = OneCycleLR(num_samples=len(X_train.index),\n","#                        batch_size=params['BATCH_SIZE'],\n","#                        max_lr=params['MAX_LR'],\n","#                        end_percentage=0.1,\n","#                        scale_percentage=None,\n","#                        maximum_momentum=0.95,\n","#                        minimum_momentum=0.85)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [11]:"},"outputs":[],"source":["# Train the model\n","train_history1 = model.fit(\n","    x = [X_train_ids, X_train_attention],\n","    y = y_train.to_numpy(),\n","    epochs = params['EPOCHS'],\n","    batch_size = params['BATCH_SIZE'],\n","    steps_per_epoch = params['NUM_STEPS'],\n","    validation_data = ([X_valid_ids, X_valid_attention], y_valid.to_numpy()),\n","    callbacks=[early_stopping],\n","    verbose=2\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [12]:"},"outputs":[],"source":["# Unfreeze distilBERT layers and make available for training\n","for layer in distilBERT.layers:\n","    layer.trainable = True\n","    \n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [13]:"},"outputs":[],"source":["# optimizer = tf.keras.optimizers.SGD(lr=5e-5, momentum=0.9, nesterov=True)\n","optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n","\n","# Recompile model after unfreezing\n","model.compile(optimizer=optimizer, \n","              loss=focal_loss(),\n","              metrics=['accuracy'])\n","\n","# Train the model\n","train_history2 = model.fit(\n","    x = [X_train_ids, X_train_attention],\n","    y = y_train.to_numpy(),\n","    epochs = params['FT_EPOCHS'],\n","    batch_size = params['BATCH_SIZE'],\n","    steps_per_epoch = params['NUM_STEPS'],\n","    validation_data = ([X_valid_ids, X_valid_attention], y_valid.to_numpy()),\n","    callbacks=[early_stopping],\n","    verbose=2\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [14]:"},"outputs":[],"source":["# # Plot learning rate schedule\n","# plt.xlabel('Training Iterations')\n","# plt.ylabel('Learning Rate')\n","# plt.title(\"One-Cycle Learning Rate Schedule\")\n","# plt.plot(one_cycle.history['lr'])\n","# plt.savefig('./lr_schedule.png', dpi=300.0, transparent=True)\n","\n","# # Log the figure\n","# experiment.log_image('./lr_schedule.png', name='One-Cycle Schedule')"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [15]:"},"outputs":[],"source":["# # Plot momentum schedule\n","# plt.xlabel('Training Iterations')\n","# plt.ylabel('Momentum')\n","# plt.title(\"Momentum Schedule\")\n","# plt.plot(one_cycle.history['momentum'])\n","# plt.savefig('./momentum_schedule.png', dpi=300.0, transparent=True)\n","\n","# # Log the figure\n","# experiment.log_image('./momentum_schedule.png', name='Momentum Schedule')"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [16]:"},"outputs":[],"source":["# Build train_history\n","history_df1 = pd.DataFrame(train_history1.history)\n","history_df2 = pd.DataFrame(train_history2.history)\n","history_df = history_df1.append(history_df2, ignore_index=True)\n","\n","# Plot training and validation loss over each epoch\n","# history_df = pd.DataFrame(train_history.history)\n","history_df.loc[:, ['loss', 'val_loss']].plot()\n","plt.title(label='Training + Validation Loss Over Time', fontsize=17, pad=19)\n","plt.xlabel('Epoch', labelpad=14, fontsize=14)\n","plt.ylabel('Binary Crossentropy Loss', labelpad=16, fontsize=14)\n","print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n","\n","# Save figure\n","plt.savefig('./trainvalloss.png', dpi=300.0, transparent=True)\n","\n","# Log the figure\n","experiment.log_image('./trainvalloss.png', name='Train Validation Loss')"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [17]:"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","\n","\n","def test_index_to_example(index):\n","    \"\"\"\"\"\"\"\"\"\n","    Takes an index of the test dataset and returns\n","    the associated string of text at that index.\n","    \n","    Input:\n","        - index:  Integer representing an index of the test dataset\n","    Output:\n","        - String of text located at that index in the test dataset\n","    \"\"\"\"\"\"\"\"\"\n","    return X_test[index]\n","\n","\n","\n","with experiment.test():\n","    # Generate predictions\n","    y_pred = model.predict([X_test_ids, X_test_attention])\n","    y_pred_thresh = np.where(y_pred >= params['POS_PROBA_THRESHOLD'], 1, 0)\n","    \n","    # Get evaluation results\n","    accuracy = accuracy_score(y_test, y_pred_thresh)\n","    auc_roc = roc_auc_score(y_test, y_pred)\n","    \n","    # Log evaluation metrics\n","    experiment.log_metrics({'Accuracy':accuracy, 'AUC-ROC':auc_roc})\n","    \n","    # Log the ROC curve\n","    fpr, tpr, thresholds = roc_curve(y_test.to_numpy(), y_pred)\n","    experiment.log_curve('ROC cuve', fpr, tpr)\n","    \n","    # Log confusion matrix\n","    experiment.log_confusion_matrix(y_test.to_list(), \n","                                    y_pred.tolist(), \n","                                    labels=['non-Toxic', 'Toxic'],\n","                                    title='Test Confusion Matrix',\n","                                    row_label='Actual Toxicity',\n","                                    column_label='Predicted Toxicity',\n","                                    index_to_example_function=test_index_to_example)\n","\n","\n","print('Accuracy:  ', accuracy)   \n","print('ROC-AUC:   ', auc_roc)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [18]:"},"outputs":[],"source":["# Plot confusion matrix\n","skplt.metrics.plot_confusion_matrix(y_test.to_list(),\n","                                    y_pred_thresh.tolist(),\n","                                    figsize=(6,6),\n","                                    text_fontsize=14)\n","plt.title(label='Test Confusion Matrix', fontsize=20, pad=17)\n","plt.xlabel('Predicted Label', labelpad=14)\n","plt.ylabel('True Label', labelpad=14)\n","\n","# Save the figure\n","plt.savefig('./confusionmatrix.png', dpi=300.0, transparent=True)\n","\n","# Log the confusion matrix\n","experiment.log_image('./confusionmatrix.png', name='Test Confusion Matrix')"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [19]:"},"outputs":[],"source":["# The save-path follows a convention used by TensorFlow Serving where the last path component \n","# (4/ here) is a version number for your model - it allows tools like Tensorflow Serving \n","# to reason about the relative freshness.\n","tf.saved_model.save(model, './4/')\n","\n","# Log final model\n","experiment.log_model(name='DistilBERT 256, 32 with Unfreezing, Dropout, Adam // Unbalanced Toxic Comment', \n","                     file_or_folder='./4/')"]},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":2,"title":"In [20]:"},"outputs":[],"source":["# End Comet.ml experiment\n","experiment.end()"]}],"metadata":{"jupytext":{"main_language":"python","text_representation":{"extension":".py","format_name":"percent"}}},"nbformat":4,"nbformat_minor":2}