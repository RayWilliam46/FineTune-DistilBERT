{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [1]:"
   },
   "outputs": [],
   "source": [
    "# !pip install comet_ml\n",
    "# !pip install -q pyyaml h5py\n",
    "# !pip install scikit-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [2]:"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from comet_ml import Experiment\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertModel, DistilBertConfig\n",
    "\n",
    "# Import utility functions\n",
    "from src.utils.train_utils import batch_encode\n",
    "from src.utils.train_utils import focal_loss\n",
    "\n",
    "# Import matplotlib\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "\n",
    "# Load training data\n",
    "X_train = pd.read_csv('data/processed/unbalanced_dataset/X_train.csv')['comment_text']\n",
    "X_valid = pd.read_csv('data/processed/unbalanced_dataset/X_valid.csv')['comment_text']\n",
    "y_train = pd.read_csv('data/processed/unbalanced_dataset/y_train.csv')['isToxic']\n",
    "y_valid = pd.read_csv('data/processed/unbalanced_dataset/y_valid.csv')['isToxic']\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv('data/processed/test_merged.csv')\n",
    "X_test = test['comment_text']\n",
    "y_test = test['isToxic']\n",
    "\n",
    "# Check data\n",
    "print('Our training data has   ', len(X_train.index), ' rows.')\n",
    "print('Our validation data has ', len(X_valid.index), ' rows.')\n",
    "print('Our test data has       ', len(X_test.index), ' rows.')\n",
    "\n",
    "\n",
    "\n",
    "# Allow us to see full text (not truncated)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Up Comet.ml Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [3]:"
   },
   "outputs": [],
   "source": [
    "# Create an experiment with your api key:\n",
    "experiment = Experiment(\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    "    project_name=\"YOUR_PROJECT_NAME\",\n",
    "    workspace=\"YOUR_WORKSPACE\",\n",
    "    auto_histogram_weight_logging=True,\n",
    "    auto_histogram_gradient_logging=True,\n",
    "    auto_histogram_activation_logging=True,\n",
    "    auto_log_co2=True,\n",
    "    log_env_details=True,\n",
    "    log_env_gpu=True,\n",
    "    log_env_cpu=True\n",
    ")\n",
    "\n",
    "# Set parameters:\n",
    "params = {'MAX_LENGTH': 128,\n",
    "          'EPOCHS': 6,\n",
    "          'LEARNING_RATE': 5e-5,\n",
    "          'FT_EPOCHS': 2,\n",
    "          'OPTIMIZER': 'adam',\n",
    "          'FL_GAMMA': 2.0,\n",
    "          'FL_ALPHA': 0.2,\n",
    "          'BATCH_SIZE': 64,\n",
    "          'NUM_STEPS': len(X_train.index) // 64,\n",
    "          'DISTILBERT_DROPOUT': 0.2,\n",
    "          'DISTILBERT_ATT_DROPOUT': 0.2,\n",
    "          'LAYER_DROPOUT': 0.2,\n",
    "          'KERNEL_INITIALIZER': 'GlorotNormal',\n",
    "          'BIAS_INITIALIZER': 'zeros',\n",
    "          'POS_PROBA_THRESHOLD': 0.5,          \n",
    "          'ADDED_LAYERS': 'Dense 256, Dense 32, Dropout 0.2',\n",
    "          'LR_SCHEDULE': '5e-5 for 6 epochs, Fine-tune w/ adam for 2 epochs @2e-5',\n",
    "          'FREEZING': 'All DistilBERT layers frozen for 6 epochs, then unfrozen for 2',\n",
    "          'CALLBACKS': '[early_stopping w/ patience=0]',\n",
    "          'RANDOM_STATE':42\n",
    "          }\n",
    "\n",
    "\n",
    "# Log parameters\n",
    "experiment.log_parameters(params)\n",
    "\n",
    "# Log data assets\n",
    "experiment.log_asset('data/processed/test_merged.csv')\n",
    "experiment.log_asset_folder('data/processed/unbalanced_dataset')\n",
    "experiment.log_dataset_info(name='Toxic Comment (Unbalanced)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [5]:"
   },
   "outputs": [],
   "source": [
    "########## Ensure reproducibility ##########\n",
    "\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(params['RANDOM_STATE'])\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(params['RANDOM_STATE'])\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(params['RANDOM_STATE'])\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed=params['RANDOM_STATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [6]:"
   },
   "outputs": [],
   "source": [
    "# DistilBERT and BERT use the same tokenizer...we use the Fast version to optimize runtime\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Encode X_train\n",
    "X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n",
    "\n",
    "# Encode X_valid\n",
    "X_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())\n",
    "\n",
    "# Encode X_test\n",
    "X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [9]:"
   },
   "outputs": [],
   "source": [
    "def build_model(transformer, max_length=params['MAX_LENGTH']):\n",
    "    \n",
    "    # Define weight initializer with a random seed to ensure reproducibility\n",
    "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=params['RANDOM_STATE']) \n",
    "    \n",
    "    # Define input layers\n",
    "    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                            name='input_ids', \n",
    "                                            dtype='int32')\n",
    "    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n",
    "                                                  name='input_attention', \n",
    "                                                  dtype='int32')\n",
    "    \n",
    "    # DistilBERT outputs a tuple where the first element is tf.Tensor of shape \n",
    "    # (batch_size, sequence_length, hidden_size=768).\n",
    "    # The tf.Tensor represents a sequence of hidden-states at the output of the \n",
    "    # last layer of the model.\n",
    "    last_hidden_states = transformer([input_ids_layer, input_attention_layer])[0]\n",
    "    \n",
    "    # We only care about DistilBERT's output for the [CLS] token, which is located\n",
    "    # at index 0.  Splicing out the [CLS] tokens gives us 2D data.\n",
    "    cls_token = last_hidden_states[:, 0, :]\n",
    "    \n",
    "    D1 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n",
    "                                 seed=params['RANDOM_STATE']\n",
    "                                )(cls_token)\n",
    "    \n",
    "    X = tf.keras.layers.Dense(256,\n",
    "                              activation='relu',\n",
    "                              kernel_initializer=weight_initializer,\n",
    "                              bias_initializer='zeros'\n",
    "                              )(D1)\n",
    "    \n",
    "    D2 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n",
    "                                 seed=params['RANDOM_STATE']\n",
    "                                )(X)\n",
    "    \n",
    "    X = tf.keras.layers.Dense(32,\n",
    "                              activation='relu',\n",
    "                              kernel_initializer=weight_initializer,\n",
    "                              bias_initializer='zeros'\n",
    "                              )(D2)\n",
    "    \n",
    "    D3 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n",
    "                                 seed=params['RANDOM_STATE']\n",
    "                                )(X)\n",
    "    \n",
    "    # Define a single node that makes up the output layer (for binary classification)\n",
    "    output = tf.keras.layers.Dense(1, \n",
    "                                   activation='sigmoid',\n",
    "                                   kernel_initializer=weight_initializer,  # CONSIDER USING CONSTRAINT\n",
    "                                   bias_initializer='zeros'\n",
    "                                   )(D3)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=params['LEARNING_RATE']), \n",
    "                  loss=focal_loss(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The bare, pretrained DistilBERT transformer model outputting raw hidden-states \n",
    "# and without any specific head on top.\n",
    "config = DistilBertConfig(dropout=params['DISTILBERT_DROPOUT'], \n",
    "                          attention_dropout=params['DISTILBERT_ATT_DROPOUT'], \n",
    "                          output_hidden_states=True)\n",
    "distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "# Freeze DistilBERT layers to preserve pretrained weights \n",
    "for layer in distilBERT.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build model\n",
    "model = build_model(distilBERT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Weights of Added Layers and Classification Head"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [10]:"
   },
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  mode='min',\n",
    "                                                  min_delta=0,\n",
    "                                                  patience=0,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "train_history1 = model.fit(\n",
    "    x = [X_train_ids, X_train_attention],\n",
    "    y = y_train.to_numpy(),\n",
    "    epochs = params['EPOCHS'],\n",
    "    batch_size = params['BATCH_SIZE'],\n",
    "    steps_per_epoch = params['NUM_STEPS'],\n",
    "    validation_data = ([X_valid_ids, X_valid_attention], y_valid.to_numpy()),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unfreeze DistilBERT and Fine-tune All Weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [12]:"
   },
   "outputs": [],
   "source": [
    "# Unfreeze DistilBERT weights to enable fine-tuning\n",
    "for layer in distilBERT.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Lower the learning rate to prevent destruction of pre-trained weights\n",
    "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
    "\n",
    "# Recompile model after unfreezing\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=focal_loss(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "train_history2 = model.fit(\n",
    "    x = [X_train_ids, X_train_attention],\n",
    "    y = y_train.to_numpy(),\n",
    "    epochs = params['FT_EPOCHS'],\n",
    "    batch_size = params['BATCH_SIZE'],\n",
    "    steps_per_epoch = params['NUM_STEPS'],\n",
    "    validation_data = ([X_valid_ids, X_valid_attention], y_valid.to_numpy()),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate Model Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [17]:"
   },
   "outputs": [],
   "source": [
    "with experiment.test():\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict([X_test_ids, X_test_attention])\n",
    "    y_pred_thresh = np.where(y_pred >= params['POS_PROBA_THRESHOLD'], 1, 0)\n",
    "    \n",
    "    # Get evaluation results\n",
    "    accuracy = accuracy_score(y_test, y_pred_thresh)\n",
    "    auc_roc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    # Log evaluation metrics\n",
    "    experiment.log_metrics({'Accuracy':accuracy, 'AUC-ROC':auc_roc})\n",
    "    \n",
    "    # Log the ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test.to_numpy(), y_pred)\n",
    "    experiment.log_curve('ROC cuve', fpr, tpr)\n",
    "\n",
    "\n",
    "print('Accuracy:  ', accuracy)   # 0.9218\n",
    "print('ROC-AUC:   ', auc_roc)    # 0.9691"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot Training and Validation Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build train_history\n",
    "history_df1 = pd.DataFrame(train_history1.history)\n",
    "history_df2 = pd.DataFrame(train_history2.history)\n",
    "history_df = history_df1.append(history_df2, ignore_index=True)\n",
    "\n",
    "# Plot training and validation loss over each epoch\n",
    "# history_df = pd.DataFrame(train_history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "plt.title(label='Training + Validation Loss Over Time', fontsize=17, pad=19)\n",
    "plt.xlabel('Epoch', labelpad=14, fontsize=14)\n",
    "plt.ylabel('Focal Loss', labelpad=16, fontsize=14)\n",
    "print(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('figures/unbalanced_trainvalloss.png', dpi=300.0, transparent=True)\n",
    "\n",
    "# Log the figure\n",
    "experiment.log_image('figures/unbalanced_trainvalloss.png', name='Train Validation Loss')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot the Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [18]:"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "skplt.metrics.plot_confusion_matrix(y_test.to_list(),\n",
    "                                    y_pred_thresh.tolist(),\n",
    "                                    figsize=(6,6),\n",
    "                                    text_fontsize=14)\n",
    "plt.title(label='Test Confusion Matrix', fontsize=20, pad=17)\n",
    "plt.xlabel('Predicted Label', labelpad=14)\n",
    "plt.ylabel('True Label', labelpad=14)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('figures/confusionmatrix.png', dpi=300.0, transparent=True)\n",
    "\n",
    "# Log the confusion matrix\n",
    "experiment.log_image('figures/confusionmatrix.png', name='Test Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## End the Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "In [19]:",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The save-path follows a convention used by TensorFlow Serving where the last path component \n",
    "# (1/ here) is a version number for your model - it allows tools like Tensorflow Serving \n",
    "# to reason about the relative freshness.\n",
    "tf.saved_model.save(model, 'models/unbalanced_model/1/')\n",
    "\n",
    "# Log final model\n",
    "experiment.log_model(name='Final Model (Unbalanced)', \n",
    "                     file_or_folder='models/unbalanced_model/1/')\n",
    "\n",
    "# End Comet.ml experiment\n",
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}