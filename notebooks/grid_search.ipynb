{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"title":"In [1]:"},"outputs":[],"source":["get_ipython().system('pip install --upgrade pip')\n","get_ipython().system('pip install comet_ml')\n","get_ipython().system('pip install -q pyyaml h5py')\n","get_ipython().system('pip install scikit-plot')"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [2]:"},"outputs":[],"source":["# Import libraries\n","from comet_ml import Experiment\n","from comet_ml import Optimizer\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","import seaborn as sns\n","import scikitplot as skplt\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import initializers\n","import tqdm\n","\n","# Import matplotlib\n","pd.plotting.register_matplotlib_converters()\n","import matplotlib.pyplot as plt\n","get_ipython().run_line_magic('matplotlib', 'inline')\n","\n","# Load training data\n","X_train = pd.read_csv('../input/toxic-comment-unbalanced/X_train.csv')['comment_text']\n","X_valid = pd.read_csv('../input/toxic-comment-unbalanced/X_valid.csv')['comment_text']\n","y_train = pd.read_csv('../input/toxic-comment-unbalanced/y_train.csv')['isToxic']\n","y_valid = pd.read_csv('../input/toxic-comment-unbalanced/y_valid.csv')['isToxic']\n","\n","# Load test data\n","test = pd.read_csv('../input/toxic-comment-test-merged/test_merged.csv')\n","X_test = test['comment_text']\n","y_test = test['isToxic']\n","\n","# Check data\n","print('Our training data has   ', len(X_train.index), ' rows.')\n","print('Our validation data has ', len(X_valid.index), ' rows.')\n","print('Our test data has       ', len(X_test.index), ' rows.')\n","\n","\n","# Allow us to see full text (not truncated)\n","pd.set_option('display.max_colwidth', None)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [3]:"},"outputs":[],"source":["# Set parameters:\n","params = {'MAX_LENGTH': 128,\n","          'EPOCHS': 6,\n","          'LEARNING_RATE': 5e-5,\n","          'OPTIMIZER': 'adam',\n","          'LOSS': 'Focal Loss // gamma=2, alpha=.8',\n","          'FL_GAMMA': 2.0,\n","          'FL_ALPHA': 0.8,\n","          'BATCH_SIZE': 64,\n","          'NUM_STEPS': X_train.shape[0] // 64,\n","          'DISTILBERT_DROPOUT': 0.2,\n","          'DISTILBERT_ATT_DROPOUT': 0.2,\n","          'LAYER_DROPOUT': 0.2,\n","          'KERNEL_INITIALIZER': 'GlorotNormal',\n","          'BIAS_INITIALIZER': 'zeros',\n","          'POS_PROBA_THRESHOLD': 0.5,\n","          'CALLBACKS': '[early_stopping]',\n","          \n","          'LR_SCHEDULE': 'None',\n","          'FREEZING': 'All distilBERT layers frozen',\n","          'OTHER': 'None',\n","          'DATASET': 'Unbalanced Splits',\n","          'RANDOM_STATE':42\n","         }\n","\n","# Define callback\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n","                                                  mode='max',\n","                                                  min_delta=0,\n","                                                  patience=2,\n","                                                  restore_best_weights=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [4]:"},"outputs":[],"source":["# DistilBERT and BERT use the same tokenizer...we use the Fast version to optimize runtime\n","from transformers import DistilBertTokenizerFast\n","\n","def batch_encode(tokenizer, sentences):\n","    \"\"\"\"\"\"\"\"\"\n","    A function that encodes a batch of sentences and returns the sentences'\n","    corresponding encodings and attention masks that are ready to be fed \n","    into a pre-trained transformer model.\n","    \n","    \n","    Input:\n","        - tokenizer:  tokenizer object from the PreTrainedTokenizer Class\n","        - sentences:  a list of strings where each string represents a sentence\n","    Output:\n","        - input_ids:       a sentence encoded as a tf.Tensor object\n","        - attention_mask:  the sentence's attention mask encoded as a tf.Tensor object\n","    \"\"\"\"\"\"\"\"\"\n","    inputs = tokenizer.batch_encode_plus(sentences,\n","                                         max_length=params['MAX_LENGTH'],\n","                                         padding='longest', # implements dynamic padding\n","                                         truncation=True,\n","                                         return_tensors='tf',\n","                                         return_attention_mask=True, \n","                                         return_token_type_ids=False)\n","    \n","    input_ids = inputs['input_ids']\n","    attention_mask = inputs['attention_mask']\n","    \n","    return input_ids, attention_mask\n","\n","# Get tokenizer\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","# Encode X_train\n","X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n","\n","# Encode X_valid\n","X_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())\n","\n","# Encode X_test\n","X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [5]:"},"outputs":[],"source":["from transformers import TFDistilBertModel, DistilBertConfig\n","\n","\n","def focal_loss(gamma=params['FL_GAMMA'], alpha=params['FL_ALPHA']):\n","    def focal_loss_fixed(y_true, y_pred):\n","        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n","        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n","        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n","    return focal_loss_fixed\n","\n","def build_model(experiment):\n","    \n","    # The bare DistilBERT encoder/transformer outputting raw hidden-states \n","    # without any specific head on top.  Enumerates default settings. \n","    config = DistilBertConfig(dropout=params['DISTILBERT_DROPOUT'], \n","                              attention_dropout=params['DISTILBERT_ATT_DROPOUT'], \n","                              output_hidden_states=True)\n","    distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n","\n","    # Make DistilBERT layers untrainable\n","    for layer in distilBERT.layers:\n","        layer.trainable = False\n","    \n","    # Define weight initializer with a random seed to ensure reproducibility\n","    weight_initializer = tf.keras.initializers.GlorotNormal(seed=params['RANDOM_STATE']) \n","    \n","    # Define input layers\n","    input_ids_layer = tf.keras.layers.Input(shape=(128,), \n","                                            name='input_ids', \n","                                            dtype='int32')\n","    input_attention_layer = tf.keras.layers.Input(shape=(128,), \n","                                                  name='input_attention', \n","                                                  dtype='int32')\n","    \n","    # DistilBERT outputs a tuple where the first element is tf.Tensor of shape \n","    # (batch_size, sequence_length, hidden_size=768).\n","    # The tf.Tensor represents a sequence of hidden-states at the output of the \n","    # last layer of the model.\n","    last_hidden_states = distilBERT([input_ids_layer, input_attention_layer])[0]\n","    \n","    # We only care about DistilBERT's output for the [CLS] token, which is located\n","    # at index 0.  Splicing out the [CLS] tokens gives us 2D data.\n","    cls_token = last_hidden_states[:, 0, :]   \n","    \n","    D1 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n","                                 seed=params['RANDOM_STATE']\n","                                )(cls_token)\n","    \n","    X = tf.keras.layers.Dense(experiment.get_parameter('Dense 1'),\n","                              activation='relu',\n","                              kernel_initializer=weight_initializer,\n","                              bias_initializer='zeros'\n","                              )(D1)\n","    \n","    D2 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n","                                 seed=params['RANDOM_STATE']\n","                                )(X)\n","    \n","    X = tf.keras.layers.Dense(experiment.get_parameter('Dense 2'),\n","                              activation='relu',\n","                              kernel_initializer=weight_initializer,\n","                              bias_initializer='zeros'\n","                              )(D2)\n","    \n","    D3 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n","                                 seed=params['RANDOM_STATE']\n","                                )(X)\n","    \n","    # Define a single node that makes up the output layer (for binary classification)\n","    output = tf.keras.layers.Dense(1, \n","                                   activation='sigmoid',\n","                                   kernel_initializer=weight_initializer,\n","                                   bias_initializer='zeros'\n","                                   )(D3)\n","    \n","    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n","    \n","    model.compile(tf.keras.optimizers.Adam(lr=params['LEARNING_RATE']), \n","                  loss=focal_loss(),\n","                  metrics=['accuracy'])\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"title":"In [6]:"},"outputs":[],"source":["########## Ensure reproducibility ##########\n","\n","\n","# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n","os.environ['PYTHONHASHSEED']=str(params['RANDOM_STATE'])\n","\n","# 2. Set `python` built-in pseudo-random generator at a fixed value\n","random.seed(params['RANDOM_STATE'])\n","\n","# 3. Set `numpy` pseudo-random generator at a fixed value\n","np.random.seed(params['RANDOM_STATE'])\n","\n","# 4. Set `tensorflow` pseudo-random generator at a fixed value\n","tf.random.set_seed(seed=params['RANDOM_STATE'])"]},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":2,"title":"In [7]:"},"outputs":[],"source":["# Define Grid-search configuration:\n","config = {\n","    # We pick the Grid Search algorithm:\n","    'algorithm': 'grid',\n","\n","    # Declare what we will be optimizing, and how:\n","    'spec': {\n","             'randomize': False,\n","             'maxCombo': 0,\n","             'metric': 'Accuracy',\n","             'gridSize': 4,\n","             'minSampleSize': len(X_train.index),\n","            },\n","    \n","    # Declare your hyperparameters in the Vizier-inspired format:\n","    'parameters': {\n","                    'Dense 1': {'type': 'discrete', \n","                                'values': [32, 64, 128, 256]},\n","                    'Dense 2': {'type': 'discrete',\n","                                'values': [32, 64, 128, 256]}\n","                  },\n","    \n","    # Optionally declare a name to associate with the search instance\n","    'name': 'Grid Search of Unbalanced Dense Layers',\n","    \n","    # The number of trials per experiment to run\n","    'trials': 1\n","}\n","\n","\n","\n","# Next, create an optimizer, passing in the config:\n","opt = Optimizer(config, api_key='TJ15ARkbpofBsCW8nh6bFXun5')\n","\n","\n","# Finally, get experiments, and train your models:\n","for experiment in opt.get_experiments(\n","        api_key=\"TJ15ARkbpofBsCW8nh6bFXun5\",\n","        project_name=\"jigsaw-toxic-comment\",\n","        workspace=\"raywilliamcs\",\n","        auto_histogram_weight_logging=True,\n","        auto_histogram_gradient_logging=True,\n","        auto_histogram_activation_logging=True,\n","        auto_log_co2=True,\n","        log_env_details=True,\n","        log_env_gpu=True,\n","        log_env_cpu=True):\n","    \n","    ############  Log Parameters and Assets:  ############\n","    \n","    # Log data assets\n","    experiment.log_asset_folder('../input/toxic-comment-test-merged')\n","    experiment.log_asset_folder('../input/toxic-comment-unbalanced')\n","    experiment.log_dataset_info(name='Toxic Comment (Unbalanced)')\n","    \n","    \n","    # Log parameters:\n","    experiment.log_parameter('Other', \n","                             'Grid Search of Unbalanced Dataset With Two Hidden Layers and Freezing')\n","    \n","    ############  Build the model:  ############\n","    model = build_model(experiment)\n","    \n","    ############  Train the model:  ############\n","    train_history = model.fit(\n","                            x = [X_train_ids, X_train_attention],\n","                            y = y_train.to_numpy(),\n","                            epochs = params['EPOCHS'],\n","                            batch_size = params['BATCH_SIZE'],\n","                            steps_per_epoch = params['NUM_STEPS'],\n","                            validation_data = ([X_valid_ids, X_valid_attention], \n","                                               y_valid.to_numpy()),\n","                            callbacks=[early_stopping],\n","                            verbose=2)\n","    \n","    # Plot training and validation loss over each epoch\n","    history_df = pd.DataFrame(train_history.history)\n","    history_df.loc[:, ['loss', 'val_loss']].plot()\n","    plt.title(label='Training + Validation Loss Over Time', fontsize=17, pad=19)\n","    plt.xlabel('Epoch', labelpad=14, fontsize=14)\n","    plt.ylabel('Focal Loss', labelpad=16, fontsize=14)\n","\n","    # Save figure\n","    plt.savefig('./trainvalloss.png', dpi=300.0, transparent=True)\n","\n","    # Log the figure\n","    experiment.log_image('./trainvalloss.png', name='Train Validation Loss')\n","    \n","    ############  Evaluate the model  ############\n","    with experiment.test():\n","        # Generate predictions\n","        y_pred = model.predict([X_test_ids, X_test_attention])\n","        y_pred_thresh = np.where(y_pred >= params['POS_PROBA_THRESHOLD'], 1, 0)\n","    \n","        # Get evaluation results\n","        accuracy = accuracy_score(y_test, y_pred_thresh)\n","        auc_roc = roc_auc_score(y_test, y_pred)\n","    \n","        # Log evaluation metrics\n","        experiment.log_metrics({'Accuracy':accuracy, 'AUC-ROC':auc_roc})\n","    \n","        # Log the ROC curve\n","        fpr, tpr, thresholds = roc_curve(y_test.to_numpy(), y_pred)\n","        experiment.log_curve('ROC cuve', fpr, tpr)\n","        \n","    \n","    ############  Plot confusion matrix  ############\n","    # Plot confusion matrix\n","    skplt.metrics.plot_confusion_matrix(y_test.to_list(),\n","                                        y_pred_thresh.tolist(),\n","                                        figsize=(6,6),\n","                                        text_fontsize=14)\n","    plt.title(label='Test Confusion Matrix', fontsize=20, pad=17)\n","    plt.xlabel('Predicted Label', labelpad=14)\n","    plt.ylabel('True Label', labelpad=14)\n","\n","    # Save the figure\n","    plt.savefig('./confusionmatrix.png', dpi=300.0, transparent=True)\n","\n","    # Log the confusion matrix\n","    experiment.log_image('./confusionmatrix.png', name='Test Confusion Matrix')\n","    \n","    \n","     ############  End experiment  ############\n","    # End Comet.ml experiment\n","    experiment.end()"]}],"metadata":{"jupytext":{"main_language":"python","text_representation":{"extension":".py","format_name":"percent"}}},"nbformat":4,"nbformat_minor":2}