{"cells":[{"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2021-01-11T01:46:20.547961Z","iopub.status.busy":"2021-01-11T01:46:20.547208Z","iopub.status.idle":"2021-01-11T01:47:02.981179Z","shell.execute_reply":"2021-01-11T01:47:02.981798Z"},"papermill":{"duration":42.453522,"end_time":"2021-01-11T01:47:02.981995","exception":false,"start_time":"2021-01-11T01:46:20.528473","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# !pip install --upgrade pip\n# !pip install comet_ml\n# !pip install -q pyyaml h5py\n# !pip install scikit-plot","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.010089,"end_time":"2021-01-11T01:46:20.519127","exception":false,"start_time":"2021-01-11T01:46:20.509038","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Initial Setup"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-11T01:47:03.048574Z","iopub.status.busy":"2021-01-11T01:47:03.047386Z","iopub.status.idle":"2021-01-11T01:47:10.111669Z","shell.execute_reply":"2021-01-11T01:47:10.111155Z"},"papermill":{"duration":7.106063,"end_time":"2021-01-11T01:47:10.11179","exception":false,"start_time":"2021-01-11T01:47:03.005727","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Import libraries\nfrom comet_ml import Experiment\nfrom comet_ml import Optimizer\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport scikitplot as skplt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import initializers\nfrom transformers import DistilBertTokenizerFast\nfrom transformers import TFDistilBertModel, DistilBertConfig\n\n# Import matplotlib\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import utility functions\nfrom src.utils.train_utils import batch_encode\nfrom src.utils.train_utils import focal_loss\n\n\n\n# Load training data\nX_train = pd.read_csv('data/processed/unbalanced_dataset/X_train.csv')['comment_text']\nX_valid = pd.read_csv('data/processed/unbalanced_dataset/X_valid.csv')['comment_text']\ny_train = pd.read_csv('data/processed/unbalanced_dataset/y_train.csv')['isToxic']\ny_valid = pd.read_csv('data/processed/unbalanced_dataset/y_valid.csv')['isToxic']\n\n# Load test data\ntest = pd.read_csv('data/processed/test_merged.csv')\nX_test = test['comment_text']\ny_test = test['isToxic']\n\n# Check data\nprint('Our training data has   ', len(X_train.index), ' rows.')\nprint('Our validation data has ', len(X_valid.index), ' rows.')\nprint('Our test data has       ', len(X_test.index), ' rows.')\n\n\n\n# Allow us to see full text (not truncated)\npd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-11T01:47:10.214223Z","iopub.status.busy":"2021-01-11T01:47:10.212732Z","iopub.status.idle":"2021-01-11T01:47:10.215957Z","shell.execute_reply":"2021-01-11T01:47:10.21544Z"},"papermill":{"duration":0.034733,"end_time":"2021-01-11T01:47:10.21605","exception":false,"start_time":"2021-01-11T01:47:10.181317","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Set parameters:\nparams = {'MAX_LENGTH': 128,\n          'EPOCHS': 6,\n          'LEARNING_RATE': 5e-5,\n          'OPTIMIZER': 'adam',\n          'LOSS': 'Focal Loss // gamma=2, alpha=.8',\n          'FL_GAMMA': 2.0,\n          'FL_ALPHA': 0.8,\n          'BATCH_SIZE': 64,\n          'NUM_STEPS': X_train.shape[0] // 64,\n          'DISTILBERT_DROPOUT': 0.2,\n          'DISTILBERT_ATT_DROPOUT': 0.2,\n          'LAYER_DROPOUT': 0.2,\n          'KERNEL_INITIALIZER': 'GlorotNormal',\n          'BIAS_INITIALIZER': 'zeros',\n          'POS_PROBA_THRESHOLD': 0.5,\n          'CALLBACKS': '[early_stopping w/ patience=2]',\n          'FREEZING': 'All distilBERT layers frozen',\n          'DATASET': 'Unbalanced Splits',\n          'RANDOM_STATE':42\n         }\n\n# Define callback\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n                                                  mode='max',\n                                                  min_delta=0,\n                                                  patience=2,\n                                                  restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-11T01:48:03.495951Z","iopub.status.busy":"2021-01-11T01:48:03.494143Z","iopub.status.idle":"2021-01-11T01:48:03.496694Z","shell.execute_reply":"2021-01-11T01:48:03.497187Z"},"papermill":{"duration":0.053075,"end_time":"2021-01-11T01:48:03.497297","exception":false,"start_time":"2021-01-11T01:48:03.444222","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def build_model(experiment):\n    \"\"\"\"\"\"\"\"\"\n    Builds a model off of the DistilBERT architecture with a\n    MAX_LENGTH of 128.  Number of nodes in the two added\n    Dense layers is configured by the Comet.ml experiment object\n    according to a grid-search.\n    \n    Input:\n      - experiment :  a Comet.ml experiment object\n    \n    Output:\n      - model:        a compiled tf.keras.Model with added classification layers \n                      on top of the base pre-trained model architecture.\n    \"\"\"\"\"\"\"\"\"\"\n    \n    # The bare, pretrained DistilBERT transformer model outputting raw hidden-states \n    # and without any specific head on top.\n    config = DistilBertConfig(dropout=params['DISTILBERT_DROPOUT'], \n                              attention_dropout=params['DISTILBERT_ATT_DROPOUT'], \n                              output_hidden_states=True)\n    distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n\n    # Make DistilBERT layers untrainable\n    for layer in distilBERT.layers:\n        layer.trainable = False\n    \n    # Define weight initializer with a random seed to ensure reproducibility\n    weight_initializer = tf.keras.initializers.GlorotNormal(seed=params['RANDOM_STATE']) \n    \n    # Define input layers\n    input_ids_layer = tf.keras.layers.Input(shape=(128,), \n                                            name='input_ids', \n                                            dtype='int32')\n    input_attention_layer = tf.keras.layers.Input(shape=(128,), \n                                                  name='input_attention', \n                                                  dtype='int32')\n    \n    # DistilBERT outputs a tuple where the first element at index 0\n    # represents the hidden-state at the output of the model's last layer.\n    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n    last_hidden_state = distilBERT([input_ids_layer, input_attention_layer])[0]\n    \n    # We only care about DistilBERT's output for the [CLS] token, which is located\n    # at index 0.  Splicing out the [CLS] tokens gives us 2D data.\n    cls_token = last_hidden_state[:, 0, :]   \n    \n    D1 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n                                 seed=params['RANDOM_STATE']\n                                )(cls_token)\n    \n    X = tf.keras.layers.Dense(experiment.get_parameter('Dense 1'),\n                              activation='relu',\n                              kernel_initializer=weight_initializer,\n                              bias_initializer='zeros'\n                              )(D1)\n    \n    D2 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n                                 seed=params['RANDOM_STATE']\n                                )(X)\n    \n    X = tf.keras.layers.Dense(experiment.get_parameter('Dense 2'),\n                              activation='relu',\n                              kernel_initializer=weight_initializer,\n                              bias_initializer='zeros'\n                              )(D2)\n    \n    D3 = tf.keras.layers.Dropout(params['LAYER_DROPOUT'],\n                                 seed=params['RANDOM_STATE']\n                                )(X)\n    \n    # Define a single node that makes up the output layer (for binary classification)\n    output = tf.keras.layers.Dense(1, \n                                   activation='sigmoid',\n                                   kernel_initializer=weight_initializer,\n                                   bias_initializer='zeros'\n                                   )(D3)\n    \n    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n    \n    model.compile(tf.keras.optimizers.Adam(lr=params['LEARNING_RATE']), \n                  loss=focal_loss(),\n                  metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-11T01:48:03.604459Z","iopub.status.busy":"2021-01-11T01:48:03.60307Z","iopub.status.idle":"2021-01-11T01:48:03.606091Z","shell.execute_reply":"2021-01-11T01:48:03.605618Z"},"papermill":{"duration":0.033753,"end_time":"2021-01-11T01:48:03.606183","exception":false,"start_time":"2021-01-11T01:48:03.57243","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"########## Ensure reproducibility ##########\n\n\n# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\nos.environ['PYTHONHASHSEED']=str(params['RANDOM_STATE'])\n\n# 2. Set `python` built-in pseudo-random generator at a fixed value\nrandom.seed(params['RANDOM_STATE'])\n\n# 3. Set `numpy` pseudo-random generator at a fixed value\nnp.random.seed(params['RANDOM_STATE'])\n\n# 4. Set `tensorflow` pseudo-random generator at a fixed value\ntf.random.set_seed(seed=params['RANDOM_STATE'])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.022479,"end_time":"2021-01-11T01:47:10.261415","exception":false,"start_time":"2021-01-11T01:47:10.238936","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Encode Datasets"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-11T01:47:10.316158Z","iopub.status.busy":"2021-01-11T01:47:10.315481Z","iopub.status.idle":"2021-01-11T01:48:03.367909Z","shell.execute_reply":"2021-01-11T01:48:03.368394Z"},"papermill":{"duration":53.084315,"end_time":"2021-01-11T01:48:03.368538","exception":false,"start_time":"2021-01-11T01:47:10.284223","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Instantiate DistilBERT tokenizer...we use the Fast version to optimize runtime\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# Encode X_train\nX_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n\n# Encode X_valid\nX_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())\n\n# Encode X_test\nX_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.028833,"end_time":"2021-01-11T01:48:03.665897","exception":false,"start_time":"2021-01-11T01:48:03.637064","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Run Grid-Search on Comet.ml"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-11T01:48:03.742Z","iopub.status.busy":"2021-01-11T01:48:03.741103Z","iopub.status.idle":"2021-01-11T07:31:58.986487Z","shell.execute_reply":"2021-01-11T07:31:58.987028Z"},"papermill":{"duration":20635.295949,"end_time":"2021-01-11T07:31:58.987171","exception":false,"start_time":"2021-01-11T01:48:03.691222","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Define Grid-search configuration:\nconfig = {\n    # We pick the Grid Search algorithm:\n    'algorithm': 'grid',\n\n    # Declare what we will be optimizing, and how:\n    'spec': {\n             'randomize': False,\n             'maxCombo': 0,\n             'metric': 'Accuracy',\n             'gridSize': 4,\n             'minSampleSize': len(X_train.index),\n            },\n    \n    # Declare your hyperparameters in the Vizier-inspired format:\n    'parameters': {\n                    'Dense 1': {'type': 'discrete', \n                                'values': [32, 64, 128, 256]},\n                    'Dense 2': {'type': 'discrete',\n                                'values': [32, 64, 128, 256]}\n                  },\n    \n    # Optionally declare a name to associate with the search instance\n    'name': 'Grid Search of Unbalanced Dense Layers',\n    \n    # The number of trials per experiment to run\n    'trials': 1\n}\n\n\n\n# Next, create an optimizer, passing in the config:\nopt = Optimizer(config, api_key='YOUR_API_KEY')\n\n\n\n# Finally, get experiments, and train your models:\nfor experiment in opt.get_experiments(\n        api_key=\"YOUR_API_KEY\",\n        project_name=\"YOUR_PROJECT_NAME\",\n        workspace=\"YOUR_WORKSPACE\",\n        auto_histogram_weight_logging=True,\n        auto_histogram_gradient_logging=True,\n        auto_histogram_activation_logging=True,\n        auto_log_co2=True,\n        log_env_details=True,\n        log_env_gpu=True,\n        log_env_cpu=True):\n    \n    \n    ############  Log Parameters and Assets:  ############\n    \n    # Log data assets\n    experiment.log_asset('data/processed/test_merged.csv')\n    experiment.log_asset_folder('data/processed/unbalanced_dataset')\n    experiment.log_dataset_info(name='Toxic Comment (Unbalanced)')\n    \n    \n    # Log custom parameter\n    experiment.log_parameter('Other', \n                             'Grid Search of Unbalanced Dataset With Two Hidden Layers and Freezing')\n    \n    \n    ############  Build the model:  ############\n    \n    model = build_model(experiment)\n    \n    \n    ############  Train the model:  ############\n    \n    train_history = model.fit(\n                            x = [X_train_ids, X_train_attention],\n                            y = y_train.to_numpy(),\n                            epochs = params['EPOCHS'],\n                            batch_size = params['BATCH_SIZE'],\n                            steps_per_epoch = params['NUM_STEPS'],\n                            validation_data = ([X_valid_ids, X_valid_attention], \n                                               y_valid.to_numpy()),\n                            callbacks=[early_stopping],\n                            verbose=2)\n    \n\n    ############  Evaluate the model  ############\n    \n    with experiment.test():\n        # Generate predictions\n        y_pred = model.predict([X_test_ids, X_test_attention])\n        y_pred_thresh = np.where(y_pred >= params['POS_PROBA_THRESHOLD'], 1, 0)\n    \n        # Get evaluation results\n        accuracy = accuracy_score(y_test, y_pred_thresh)\n        auc_roc = roc_auc_score(y_test, y_pred)\n    \n        # Log evaluation metrics\n        experiment.log_metrics({'Accuracy':accuracy, 'AUC-ROC':auc_roc})\n    \n        # Log the ROC curve\n        fpr, tpr, thresholds = roc_curve(y_test.to_numpy(), y_pred)\n        experiment.log_curve('ROC cuve', fpr, tpr)\n        \n    \n    ############  Plot Train and Validation Loss  ############\n        \n    # Plot training and validation loss over each epoch\n    history_df = pd.DataFrame(train_history.history)\n    history_df.loc[:, ['loss', 'val_loss']].plot()\n    plt.title(label='Training + Validation Loss Over Time', fontsize=17, pad=19)\n    plt.xlabel('Epoch', labelpad=14, fontsize=14)\n    plt.ylabel('Focal Loss', labelpad=16, fontsize=14)\n\n    # Save figure\n    plt.savefig('figures/unbalanced_trainvalloss.png', dpi=300.0, transparent=True)\n\n    # Log the figure\n    experiment.log_image('figures/unbalanced_trainvalloss.png', name='Train Validation Loss')\n    \n    \n    ############  Plot confusion matrix  ############\n    \n    # Plot confusion matrix\n    skplt.metrics.plot_confusion_matrix(y_test.to_list(),\n                                        y_pred_thresh.tolist(),\n                                        figsize=(6,6),\n                                        text_fontsize=14)\n    plt.title(label='Test Confusion Matrix', fontsize=20, pad=17)\n    plt.xlabel('Predicted Label', labelpad=14)\n    plt.ylabel('True Label', labelpad=14)\n\n    # Save the figure\n    plt.savefig('figures/unbalanced_confusionmatrix.png', dpi=300.0, transparent=True)\n\n    # Log the confusion matrix\n    experiment.log_image('figures/unbalanced_confusionmatrix.png', name='Test Confusion Matrix')\n    \n    \n     ############  End experiment  ############\n        \n    # End Comet.ml experiment\n    experiment.end()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}